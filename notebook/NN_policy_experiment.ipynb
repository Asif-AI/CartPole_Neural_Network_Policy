{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\LIVE_CLASS\\FSDS_FEB\\codebase\\FSDS-Demo-CartPole-NN-policy\\env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [\n",
    "    tf.keras.layers.Dense(5, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # left prob # 1 < left , 0 > right\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg = np.array([1,2,3])\n",
    "eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_policy(observation, model): # policy gradient -> PG\n",
    "    left_probability = model.predict(observation[np.newaxis]) # probability value between 0, and 1\n",
    "    action = int(np.random.rand() > left_probability) # value {0, 1} # exploration vs exploitation concept\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "Optimize learnable parameters of policy by following the gradients towards higher reward (maximizing reward)\n",
    "\n",
    "## steps\n",
    "1. let the NN play the game multiple times and at every step just calculate the gradients (wrt reward) but dont apply it immidiately.\n",
    "2. Once you have completed several episodes then compute the actions using discounted method.\n",
    "3. result of previous step 2 can +ve or -ve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.6645621]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.uniform([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, observation, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prabability = model(observation[np.newaxis])\n",
    "        action = (tf.random.uniform([1,1]) > left_prabability) # True and False\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) # \n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prabability)) \n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables) # dc/dw\n",
    "    new_observation, reward, done, info = env.step(int(action))\n",
    "    return new_observation, reward, done, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = list()\n",
    "    all_grads = list()\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = list()\n",
    "        current_grads = list()\n",
    "        observation = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            observation, reward, done, grads = play_one_step(env, observation, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    N = len(rewards)\n",
    "    for step in range(N - 2, -1, -1):\n",
    "        # a3 + a4*gamma\n",
    "        discounted[step] = discounted[step] + discounted[step + 1] * discount_factor\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [10, 0, -50]\n",
    "discount_rewards(arr, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2])\n",
    "np.concatenate([x,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = list()\n",
    "    for reward in all_rewards:\n",
    "        # discounted rewards\n",
    "        drs = discount_rewards(reward, discount_factor)\n",
    "        all_discounted_rewards.append(drs)\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "\n",
    "    normalize_rewards = list()\n",
    "    for discounted_rewards in all_discounted_rewards:\n",
    "        nrs = (discounted_rewards - reward_mean) / reward_std\n",
    "        normalize_rewards.append(nrs)\n",
    "    return normalize_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(seed=SEED)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = [1,2,3]\n",
    "r2 = [-1,-2,3]\n",
    "all_rewards_1 = [r1, r2]\n",
    "list(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(sum, all_rewards_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 3, 4])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [[1,2,3], [3,4,5]]\n",
    "tf.reduce_mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1/150 mean rewards: 18.6\n",
      "Iteration: 2/150 mean rewards: 21.2\n",
      "Iteration: 3/150 mean rewards: 16.2\n",
      "Iteration: 4/150 mean rewards: 20.1\n",
      "Iteration: 5/150 mean rewards: 22.3\n",
      "Iteration: 6/150 mean rewards: 24.0\n",
      "Iteration: 7/150 mean rewards: 17.6\n",
      "Iteration: 8/150 mean rewards: 18.7\n",
      "Iteration: 9/150 mean rewards: 20.9\n",
      "Iteration: 10/150 mean rewards: 19.3\n",
      "Iteration: 11/150 mean rewards: 22.7\n",
      "Iteration: 12/150 mean rewards: 21.9\n",
      "Iteration: 13/150 mean rewards: 20.2\n",
      "Iteration: 14/150 mean rewards: 23.7\n",
      "Iteration: 15/150 mean rewards: 19.6\n",
      "Iteration: 16/150 mean rewards: 22.4\n",
      "Iteration: 17/150 mean rewards: 25.2\n",
      "Iteration: 18/150 mean rewards: 19.3\n",
      "Iteration: 19/150 mean rewards: 23.3\n",
      "Iteration: 20/150 mean rewards: 21.0\n",
      "Iteration: 21/150 mean rewards: 29.7\n",
      "Iteration: 22/150 mean rewards: 39.9\n",
      "Iteration: 23/150 mean rewards: 22.3\n",
      "Iteration: 24/150 mean rewards: 24.5\n",
      "Iteration: 25/150 mean rewards: 31.0\n",
      "Iteration: 26/150 mean rewards: 34.8\n",
      "Iteration: 27/150 mean rewards: 31.1\n",
      "Iteration: 28/150 mean rewards: 27.0\n",
      "Iteration: 29/150 mean rewards: 26.1\n",
      "Iteration: 30/150 mean rewards: 30.4\n",
      "Iteration: 31/150 mean rewards: 43.6\n",
      "Iteration: 32/150 mean rewards: 36.9\n",
      "Iteration: 33/150 mean rewards: 29.7\n",
      "Iteration: 34/150 mean rewards: 27.8\n",
      "Iteration: 35/150 mean rewards: 25.9\n",
      "Iteration: 36/150 mean rewards: 29.1\n",
      "Iteration: 37/150 mean rewards: 38.0\n",
      "Iteration: 38/150 mean rewards: 40.8\n",
      "Iteration: 39/150 mean rewards: 45.7\n",
      "Iteration: 40/150 mean rewards: 46.0\n",
      "Iteration: 41/150 mean rewards: 31.4\n",
      "Iteration: 42/150 mean rewards: 35.4\n",
      "Iteration: 43/150 mean rewards: 48.7\n",
      "Iteration: 44/150 mean rewards: 41.2\n",
      "Iteration: 45/150 mean rewards: 35.0\n",
      "Iteration: 46/150 mean rewards: 44.9\n",
      "Iteration: 47/150 mean rewards: 44.8\n",
      "Iteration: 48/150 mean rewards: 43.6\n",
      "Iteration: 49/150 mean rewards: 40.3\n",
      "Iteration: 50/150 mean rewards: 50.5\n",
      "Iteration: 51/150 mean rewards: 51.3\n",
      "Iteration: 52/150 mean rewards: 33.3\n",
      "Iteration: 53/150 mean rewards: 31.3\n",
      "Iteration: 54/150 mean rewards: 41.0\n",
      "Iteration: 55/150 mean rewards: 63.3\n",
      "Iteration: 56/150 mean rewards: 55.6\n",
      "Iteration: 57/150 mean rewards: 57.7\n",
      "Iteration: 58/150 mean rewards: 43.1\n",
      "Iteration: 59/150 mean rewards: 37.1\n",
      "Iteration: 60/150 mean rewards: 53.8\n",
      "Iteration: 61/150 mean rewards: 43.3\n",
      "Iteration: 62/150 mean rewards: 49.3\n",
      "Iteration: 63/150 mean rewards: 50.1\n",
      "Iteration: 64/150 mean rewards: 54.3\n",
      "Iteration: 65/150 mean rewards: 62.2\n",
      "Iteration: 66/150 mean rewards: 54.2\n",
      "Iteration: 67/150 mean rewards: 63.0\n",
      "Iteration: 68/150 mean rewards: 59.7\n",
      "Iteration: 69/150 mean rewards: 46.7\n",
      "Iteration: 70/150 mean rewards: 48.8\n",
      "Iteration: 71/150 mean rewards: 62.6\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn\n",
    "    )\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"Iteration: {iteration + 1}/{n_iterations}\",\n",
    "    f\"mean rewards: {total_rewards/n_episodes_per_update}\"\n",
    "    )\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "\n",
    "    all_mean_grads = list()\n",
    "    # Weight of 5 hidden nodes, bias for 5 nodes, w for output node, bias for output node\n",
    "    N = len(model.trainable_variables)\n",
    "    for var_index in range(N):\n",
    "        temp_reduce_mean = list()\n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards): # rewards for every episode\n",
    "            for step, final_reward in enumerate(final_rewards):\n",
    "                result = final_reward * all_grads[episode_index][step][var_index]\n",
    "                temp_reduce_mean.append(result)\n",
    "        mean_grads = tf.reduce_mean(temp_reduce_mean, axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "unique_name = re.sub(r\"[\\s+:]\", \"_\", time.asctime())\n",
    "model_name = f\"model_at_{unique_name}_.h5\"\n",
    "model.save(model_name)\n",
    "print(f\"model is saved as '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, -1), (2, -2), (3, 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = [1,2,3]\n",
    "r2 = [-1,-2,3]\n",
    "list(zip(r1, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6882a651a5f0a61c62c98bad0141cd5bd2e13c79648f5471490eafda34f8c34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
